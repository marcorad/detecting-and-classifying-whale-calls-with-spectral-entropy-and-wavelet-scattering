# General Reponse and Notes for Reviewers

- The original purpose of this paper is to show how WS can be used as a convenient TF decomposition to perform detection and classification simultaneously, thereby combining the methods of two usually separate systems. We have revised some parts that makes this goal absolutely clear, so that we do not misinform the reader that WS coefficients can provide better detection results compared to STFT-based detectors. It is comparatively similar to answering the question "what if we can do detection and classification with MFCCs simulataneously?". 
- Our original codebase is in MATLAB, with some inefficient implementations. THe organisation of the codebase will also be difficult to follow for readers. As such, we have opted to redo our entire codebase (originally in MATLAB) in python for public availability. In the new codebase, we follow the scattering computations optimally similar to Kymatio implementations, which is reflected in the paper.
- Since we have switched our implementation language, note that the figures will have different style than in the first version provided.
- Our detection method of utilising GMMs to fit noise and signal levels in the entropy has been reinvistigated. There is a massive limitation of this technique: if there are no significant signals present, this technique will produce gibberish output, as the GMM fit will then attempt to fit 2 Gaussians to the distribution of the noise entropy level only. We have taken some ideas for the suggested paper by reviewer 2, Helble et. al. (2012), to perform a robust distribution estimate of the noise entropy, and then provide detector output as an hypothesis test: "what is the probability that the observed entropy measure is drawn from the noise entropy distribution?", thereby achieving the same result with single PDF instead of 2 Gaussians as is the case by the GMM fit. TThe proposed method therefore remains very similar to what the GMM tries to achieve, which aims to provide a probabilistic output based on a test statistic - spectral entropy in this case.
- Regarding the state of the dataset, we do not have the time or expertise to repair 187 hours of audio annotations, as it is not within the intended scope of this paper. Furthermore, the SNR of many calls can be very low, so even experienced analysts may have trouble with positive identification. Our initial intension was to evaluate the open-source dataset as-is, thereby highlighting the difficulty of bio-acoustic real-world datasets, without biasing our methods to data pre-selection. However, we understand that it is very difficult to substantiate our methods without a reliable dataset. As such, we have opted limit the dataset to a subset of 20 hours of audio, in which the annotations are accurate. We have chosen the audio such that it provides a full representation of the dataset - the entire year of recordings are covered (more details in the revised paper). We have repaired any obvious misidentifications, false positives or calls missed by analysts, and limited our scope to Blue whale calls only (`Bm-Ant-A/B/Z` and `Bm-D` classes). 
- We provide our repaired dataset subset as supplementary materials for this paper.
- We have changed the way we evaluate our classification performance to be more realistic to real-world training sets. First we optimise $\gamma$ on the training data according to a specified cost function. We then choose $\alpha$ (which balances false positives with missed detections) such that we get a fixed noise rejection ratio in the training set (99% of noise samples are rejected). Additionally, we sample the from the dataset differently to be more in line with how a human analyst may provide training data: we randomly choose a specified amount of hours of audio uniformly across the dataset (as an analyst would do to train a model) in blocks of 15 minutes, and provide all the noise and signal detections from those hours as training data to the models. We then evaluate our models with the remaining audio.

# Reviewer 1
| Comment | Response |
| ------- | -------- |
| No source code for WS implementation provided. | We have provided our re-written source code in the form of a public GitHub repository. 
| Critical downsampling ambiguity: original implementations are path dependent, whereas the custom implementation is not (ambiguous). | Our original implementation only critically downsamples the largest $\lambda$ optimally, and does not perform optimal computations. See the general response notes for the new implementation and codebase. |
| Incorrect description of admissibility criterion. | We have clarified this definition. |
| $\psi$ is used for level 1 and 2 WS, must be clarified. | We changed the notation for clarity. |
| Eqn. (10) unused in the publication. | We have removed this equation. |
| Provide the unit of the invariance scale $T$. | Our original implementation provides confusion here, since the factor $2\pi$ is simply a bandwidth defining parameter in the frequency domain only. We have clarified these definitions all in favor of specifying a downsample factor $d$ instead of an invariance scale for clarity (WS implementations, such as MATLAB and Kymatio, can have unclear definitions of LPF bandwidth to invariance scale $T$). Refer to the paper for exact details, which is also based on another paper we have submitted to IEEE Signal Processing Letters (pre-print cited).
| "Information leakage" is an imprecise description of time-support exceeding $T$. | We have provided a precise definition of this. |
| Spectral entropy with WS is interesting, highlight this section. | We have expanded on this section, including discussions on the benefits over the FFT. |
| Siddiqui (1962), "Some problems connected with Rayleigh distributions": discuss statistical implications of amplitude spectra in whitening, with Liutkus and Badeau (2015), "Generalized Wiener filtering with fractional power spectrograms", as a possible favourable argument for our use case. | We have included the requested theory, and provided discussions on how this links to our whitening procedures. |
| Eqn. (19) should link with a discussion of PCEN. | We have provided a discussion of PCEN, with Lostanlen et. al. (2019), "Per-Channel Energy Normalization: Why and How" as reference. |
| Remove dogmatic bias against neural networks. | We have instead opted to provide a more holistic discussion of the current state of the art of NNs, along with their techniques and features, and how it can still be limiting in the world of bioacoustics, depending on the use-case. |
| Describe the features used as input by NNs, which is not just "mostly MFCCs", as this seems biased against NNs. | See the response above. | 
| Fig. 6 should expand the acronyms in the caption. | We have expanded the acronyms. |

# Reviewer 2
| Comment | Response |
| ------- | -------- |
| Detector algorithm (automatic entropy scaling with GMMs) seems overly complex. | We have reconsidered our method, and opted to simplify it by considering only the distribution of the of noise entropy level, thereby still providing probabilistic output, along with median filtering as a method to stabilise the "step responses" produced by signals. Please see the general comments section on the reconsidered change. |
| Perfomance of BLED with adaptive thresholding and whitening. | We have incorporated our probabilistic adaptive thresholding (distribution of the noise energy level) and included the results. |
| Describe the gains obtained by WS instead of the standard FFT/STFT. | We have provided a more in-depth discussion of the gains obtained from WS versus the STFT for entropy/energy calculation. However, it is possible also post-process the STFT to obtain similar benefits. The main benefit is that we can perform detection directly on WS features, which is then ready to pipe into a classification system. If we wanted to use WS features for classification, we would otherwise first have to perform detection with STFT for power-law detectors, after which we then need to compute the WS features. |
| Computational complexity of WS versus STFT | We have included the computational complexity metrics. WS is undoubtedly slower than the STFT, but can provide superior classification performance in certain settings compared to STFT-based features (such as MFCCs).|
| Benchmark the detectors against Nuttall (1994), "Detection performance of power-law processors for random signals of unknown location, structure, extent, and strength", Nuttall (1996), "Near-optimum detection performance of power-law processors for random signals of unknown locations, structure, extent, and arbitrary strengths" and Helble et. al. (2012), "A generalized power-law detection lgorithm for humpback whale vocalizations" | We have included these detectors in our comparisons. |
| Precision/recall along with TPR/FPR, following Hildebrand et. al. (2022), "Performance metrics for marine mammal signal detection and classification". | We have included a precision/recall curve along with our TPR?FPR curve. | 
| Reconsider the GMM thresholding technique, it seems to require hyper-parameter tuning and the "fudging" of distributions. Perhaps use target SNR for adaptive threshold? | The GMM fitting does not require any hyperparameters - the calculated spectral entropy (or any other test statistic with high and low noise/signal levels) is used as independent samples from a distribution, after which two Gaussians are fitted to the levels. It is computatationally intensive, but is ultimately very simple in terms of input/output. It has some other problems (see the general notes), so we adapted to GMM approach to a single hypothesis test, which is very similar to what the GMM does, except with a single PDF. |
| The classification section description is lacking. LDA is vague, with chi-square feature selection requiring clarification. | We have included more detail in this section. |
| Take the time to repair repair the dataset, as it does not appear that it would take terrible long, so as to strengthen the conclusions of the classification section. | See the general comments section. |
| The hyper-parameters $\alpha, \gamma$ does not appear stable as a function of the training size. It does not seem to bode well for robustness. How important is the value of the parameter $\gamma$? | $\gamma$ acts as a regularisation parameter. LDA assumes each class is distributed according to $p(X\|Y=y) \sim \mathcal{N}(\mu_y, \Sigma)$, with $X$ data RV and $y$ the specific class label. $\gamma$ regularizes $\Sigma$ by decreasing feature covariances, thereby enforcing more "independence" between features and whitening the Gaussian. Varying of $\gamma$ may work better for certain classes, depending on how many samples there are and how the classes are separated. Note that we have an LDA model for each class in the paper, and $\gamma$ does not significantly jump around within classes. See the general note section for details on how we changed our evaluation of the classification section. |
| Peer reviewed reference required for Anne and Mallat (2013). | We included this reference. |
| Lines 417-418 are incomprehensible. | We provide more details on the $\chi^2$ feature selection process.
| Mathematical notation for WS is difficult to follow. For example, define $*$ as the convolution operator. | We have changed notation to provide more clarity, in line with reviewer 1's comments. |
| Replace figure 3 with S1 and S2 scattering amplitudes for one or two calls to be more instructive. | We have replaced fig. 3 with S1 and S2 amplitudes for a chosen illustrative `Bm-D` and `Bm-Ant` calls. |
| The GMM section is incremental to a publication of the same authors, so keep the details at a minimum. | We have reduced our discussions of the techniques in our previous publication. |
| The paper is long and can lose focus at times. | We have condensed as much material as possible in favour of a shorter paper. |